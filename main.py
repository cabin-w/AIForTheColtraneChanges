import os
import librosa
import numpy as np
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

# éŸ³ä¹ç†è®ºé…ç½®
CLUSTER_MAP = {
    # å’Œå¼¦ä»…ä¸ºæ¼”ç¤ºï¼Œæœªå…·ä½“åˆ†æ
    0: {"key": "C", "chords": ["Cmaj7", "G7", "Am7"]},
    1: {"key": "G", "chords": ["Gmaj7", "D7", "Em7"]},
    2: {"key": "D", "chords": ["Dmaj7", "A7", "Bm7"]},
    3: {"key": "A", "chords": ["Amaj7", "E7", "F#m7"]},
    4: {"key": "E", "chords": ["Emaj7", "B7", "C#m7"]},
    5: {"key": "B", "chords": ["Bmaj7", "F#7", "G#m7"]},
    6: {"key": "F#", "chords": ["F#maj7", "C#7", "D#m7"]},
    7: {"key": "Db", "chords": ["Dbmaj7", "Ab7", "Bdim7"]},
    8: {"key": "Ab", "chords": ["Abmaj7", "Eb7", "Fm7"]},
    9: {"key": "Eb", "chords": ["Ebmaj7", "Bb7", "Cdim7"]},
    10: {"key": "Bb", "chords": ["Bbmaj7", "F7", "Gdim7"]},
    11: {"key": "F", "chords": ["Fmaj7", "C7", "Ddim7"]},
}


def analyze_giant_steps(audio_path):
    try:
        # 1. åŠ è½½éŸ³é¢‘
        y, sr = librosa.load(audio_path, sr=44100, mono=True)
        if isinstance(y, np.ndarray):  # ç¡®ä¿åŠ è½½çš„æ˜¯ ndarray ç±»å‹
            print(f"âœ… éŸ³é¢‘åŠ è½½æˆåŠŸ | æ—¶é•¿: {librosa.get_duration(y=y, sr=sr):.2f}ç§’")
        else:
            raise ValueError("éŸ³é¢‘åŠ è½½å¤±è´¥ï¼Œè¿”å›çš„ä¸æ˜¯ ndarray ç±»å‹")

        # 2. èŠ‚æ‹æ£€æµ‹
        tempo, beat_frames = librosa.beat.beat_track(
            y=y, sr=sr, units='frames', tightness=100, trim=False
        )
        tempo = tempo if isinstance(tempo, float) else float(tempo[0])  # ç¡®ä¿tempoæ˜¯ä¸€ä¸ªæµ®åŠ¨æ•°å­—
        beat_times = librosa.frames_to_time(beat_frames, sr=sr)
        print(f"ğŸ¶ èŠ‚æ‹æ£€æµ‹å®Œæˆ | BPM: {tempo:.1f} | èŠ‚æ‹æ•°: {len(beat_times)}")

        # 3. å’Œå£°åˆ†æ
        chroma = librosa.feature.chroma_cqt(y=y, sr=sr, bins_per_octave=36, tuning=0)
        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)
        tonnetz = librosa.feature.tonnetz(y=y, sr=sr)

        # 4. å¢å¼ºç‰¹å¾æå–
        chroma_mfcc_tonnetz = np.concatenate([chroma, mfcc, tonnetz], axis=0)

        # 5. ç‰¹å¾æ ‡å‡†åŒ–
        scaler = StandardScaler()
        chroma_mfcc_tonnetz_norm = scaler.fit_transform(chroma_mfcc_tonnetz.T).T

        # 6. ä½¿ç”¨ PCA é™ç»´ï¼ˆæœ‰åŠ©äºèšç±»çš„æ•ˆæœï¼‰
        pca = PCA(n_components=10)  # é™åˆ° 10 ç»´
        chroma_mfcc_tonnetz_pca = pca.fit_transform(chroma_mfcc_tonnetz_norm.T)

        # 7. èšç±»åˆ†æ
        n_clusters = len(CLUSTER_MAP)  # ä½¿ç”¨ CLUSTER_MAP ä¸­çš„èšç±»æ•°é‡
        kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=42)
        kmeans.fit(chroma_mfcc_tonnetz_pca)

        # è·å–èšç±»æ ‡ç­¾
        labels = kmeans.labels_

        # 8. ç”Ÿæˆæ—¶é—´è½´æ•°æ®
        changes = []
        for t in beat_times:
            # è·å–èšç±»ç»“æœ
            frame_idx = librosa.time_to_frames([t], sr=sr)[0]
            cluster = labels[frame_idx]

            # è·å–å½“å‰å’Œå¼¦çš„è°ƒæ€§å’Œå’Œå¼¦
            cluster_info = CLUSTER_MAP.get(cluster, None)
            if cluster_info is None:
                continue

            current_key = cluster_info["key"]
            chord = cluster_info["chords"][0]  # ç®€åŒ–ä¸ºå–ç¬¬ä¸€ä¸ªå’Œå¼¦

            changes.append({
                "time": round(t, 2),
                "key": current_key,
                "chord": chord
            })

        return changes

    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {str(e)}")
        return []


if __name__ == "__main__":
    AUDIO_FILE = "giant_steps.mp3"

    if not os.path.exists(AUDIO_FILE):
        print(f"âš ï¸ é”™è¯¯ï¼šæ‰¾ä¸åˆ°éŸ³é¢‘æ–‡ä»¶ {AUDIO_FILE}")
        exit(1)

    result = analyze_giant_steps(AUDIO_FILE)

    if result:
        print("\n// è‡ªåŠ¨åˆ†æç»“æœ")
        print("CHORD_CHANGES = [")
        for c in result:
            print(f"  {{ time: {c['time']:.2f}, key: '{c['key']}', chord: '{c['chord']}' }},")
        print("];")
    else:
        print("âš ï¸ æœªç”Ÿæˆæœ‰æ•ˆåˆ†æç»“æœ")
